{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "import importlib\n",
    "import time\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import copy\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import timeit\n",
    "import customDataset\n",
    "from customDataset import CustomDataset\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n"
     ]
    }
   ],
   "source": [
    "class ChunkSampler(sampler.Sampler):\n",
    "    \"\"\"Samples elements sequentially from some offset. \n",
    "    Arguments:\n",
    "        num_samples: # of desired datapoints\n",
    "        start: offset where we should start selecting from\n",
    "    \"\"\"\n",
    "    def __init__(self, num_samples, start = 0):\n",
    "        self.num_samples = num_samples\n",
    "        self.start = start\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(range(self.start, self.start + self.num_samples))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    print(\"Using CUDA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 151 images under train\n",
      "Loaded 25 images under val\n",
      "Loaded 25 images under test\n",
      "Classes: \n",
      "['0', '1', '2', '3', '4', '5']\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'hand_labels_synth\\\\vgg-train'\n",
    "TRAIN = 'train'\n",
    "VAL = 'val'\n",
    "TEST = 'test'\n",
    "\n",
    "# VGG-19 Takes 224x224 images as input, so we resize all of them\n",
    "data_transforms = {\n",
    "    TRAIN: transforms.Compose([\n",
    "        # Data augmentation is a good practice for the train set\n",
    "        # Here, we randomly crop the image to 224x224 and\n",
    "        # randomly flip it horizontally. \n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    VAL: transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    TEST: transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "}\n",
    "\n",
    "image_datasets = {\n",
    "    x: dset.ImageFolder(\n",
    "        osp.join(data_dir, x), \n",
    "        transform=data_transforms[x]\n",
    "    )\n",
    "    for x in [TRAIN, VAL, TEST]\n",
    "}\n",
    "\n",
    "dataloaders = {\n",
    "    x: torch.utils.data.DataLoader(\n",
    "        image_datasets[x], batch_size=5,\n",
    "        shuffle=True, num_workers=4\n",
    "    )\n",
    "    for x in [TRAIN, VAL, TEST]\n",
    "}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in [TRAIN, VAL, TEST]}\n",
    "\n",
    "for x in [TRAIN, VAL, TEST]:\n",
    "    print(\"Loaded {} images under {}\".format(dataset_sizes[x], x))\n",
    "    \n",
    "print(\"Classes: \")\n",
    "class_names = image_datasets[TRAIN].classes\n",
    "print(image_datasets[TRAIN].classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_loader = torch.utils.data.DataLoader(dataset=image_datasets,\n",
    "#                                            batch_size=6, \n",
    "#                                            shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dtype = torch.FloatTensor # the CPU datatype\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "# This is a little utility that we'll use to reset the model\n",
    "# if we want to re-initialize all our parameters\n",
    "def reset(m):\n",
    "    if hasattr(m, 'reset_parameters'):\n",
    "        m.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.size() # read in N, C, H, W\n",
    "        return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_model(vgg, criterion, optimizer, scheduler, num_epochs=10):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(vgg.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    avg_loss = 0\n",
    "    avg_acc = 0\n",
    "    avg_loss_val = 0\n",
    "    avg_acc_val = 0\n",
    "    \n",
    "    train_batches = len(dataloaders[TRAIN])\n",
    "    val_batches = len(dataloaders[VAL])\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch {}/{}\".format(epoch, num_epochs))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        loss_train = 0\n",
    "        loss_val = 0\n",
    "        acc_train = 0\n",
    "        acc_val = 0\n",
    "        \n",
    "        vgg.train(True)\n",
    "        \n",
    "        for i, data in enumerate(dataloaders[TRAIN]):\n",
    "            if i % 100 == 0:\n",
    "                print(\"\\rTraining batch {}/{}\".format(i, train_batches / 2), end='', flush=True)\n",
    "                \n",
    "            # Use half training dataset\n",
    "            if i >= train_batches / 2:\n",
    "                break\n",
    "                \n",
    "            inputs, labels = data\n",
    "            \n",
    "            if use_gpu:\n",
    "                inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "            else:\n",
    "                inputs, labels = Variable(inputs), Variable(labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = vgg(inputs)\n",
    "            \n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_train += loss.data\n",
    "            acc_train += torch.sum(preds == labels.data)\n",
    "            \n",
    "            del inputs, labels, outputs, preds\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        print()\n",
    "        # * 2 as we only used half of the dataset\n",
    "        avg_loss = loss_train * 2 / dataset_sizes[TRAIN]\n",
    "        avg_acc = acc_train * 2 / dataset_sizes[TRAIN]\n",
    "        \n",
    "        vgg.train(False)\n",
    "        vgg.eval()\n",
    "            \n",
    "        for i, data in enumerate(dataloaders[VAL]):\n",
    "            if i % 100 == 0:\n",
    "                print(\"\\rValidation batch {}/{}\".format(i, val_batches), end='', flush=True)\n",
    "                \n",
    "            inputs, labels = data\n",
    "            \n",
    "            if use_gpu:\n",
    "                inputs, labels = Variable(inputs.cuda(), volatile=True), Variable(labels.cuda(), volatile=True)\n",
    "            else:\n",
    "                inputs, labels = Variable(inputs, volatile=True), Variable(labels, volatile=True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = vgg(inputs)\n",
    "            \n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss_val += loss.data\n",
    "            acc_val += torch.sum(preds == labels.data)\n",
    "            \n",
    "            del inputs, labels, outputs, preds\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        avg_loss_val = loss_val / dataset_sizes[VAL]\n",
    "        avg_acc_val = acc_val / dataset_sizes[VAL]\n",
    "        \n",
    "        print()\n",
    "        print(\"Epoch {} result: \".format(epoch))\n",
    "        print(\"Avg loss (train): {:.4f}\".format(avg_loss))\n",
    "        print(\"Avg acc (train): {:.4f}\".format(avg_acc))\n",
    "        print(\"Avg loss (val): {:.4f}\".format(avg_loss_val))\n",
    "        print(\"Avg acc (val): {:.4f}\".format(avg_acc_val))\n",
    "        print('-' * 10)\n",
    "        print()\n",
    "        \n",
    "        if avg_acc_val > best_acc:\n",
    "            best_acc = avg_acc_val\n",
    "            best_model_wts = copy.deepcopy(vgg.state_dict())\n",
    "        \n",
    "    elapsed_time = time.time() - since\n",
    "    print()\n",
    "    print(\"Training completed in {:.0f}m {:.0f}s\".format(elapsed_time // 60, elapsed_time % 60))\n",
    "    print(\"Best acc: {:.4f}\".format(best_acc))\n",
    "    \n",
    "    vgg.load_state_dict(best_model_wts)\n",
    "    return vgg\n",
    "\n",
    "def eval_model(vgg, criterion):\n",
    "    since = time.time()\n",
    "    avg_loss = 0\n",
    "    avg_acc = 0\n",
    "    loss_test = 0\n",
    "    acc_test = 0\n",
    "    \n",
    "    test_batches = len(dataloaders[TEST])\n",
    "    print(\"Evaluating model\")\n",
    "    print('-' * 10)\n",
    "    \n",
    "    for i, data in enumerate(dataloaders[TEST]):\n",
    "        if i % 100 == 0:\n",
    "            print(\"\\rTest batch {}/{}\".format(i, test_batches), end='', flush=True)\n",
    "\n",
    "        vgg.train(False)\n",
    "        vgg.eval()\n",
    "        inputs, labels = data\n",
    "\n",
    "        if use_gpu:\n",
    "            inputs, labels = Variable(inputs.cuda(), volatile=True), Variable(labels.cuda(), volatile=True)\n",
    "        else:\n",
    "            inputs, labels = Variable(inputs, volatile=True), Variable(labels, volatile=True)\n",
    "\n",
    "        outputs = vgg(inputs)\n",
    "\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss_test += loss.data\n",
    "        acc_test += torch.sum(preds == labels.data)\n",
    "\n",
    "        del inputs, labels, outputs, preds\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    avg_loss = loss_test / dataset_sizes[TEST]\n",
    "    avg_acc = acc_test / dataset_sizes[TEST]\n",
    "    \n",
    "    elapsed_time = time.time() - since\n",
    "    print()\n",
    "    print(\"Evaluation completed in {:.0f}m {:.0f}s\".format(elapsed_time // 60, elapsed_time % 60))\n",
    "    print(\"Avg loss (test): {:.4f}\".format(avg_loss))\n",
    "    print(\"Avg acc (test): {:.4f}\".format(avg_acc))\n",
    "    print('-' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/25\n",
      "----------\n",
      "Training batch 0/15.5\n",
      "Validation batch 0/5\n",
      "Epoch 0 result: \n",
      "Avg loss (train): 0.3774\n",
      "Avg acc (train): 0.0000\n",
      "Avg loss (val): 0.3510\n",
      "Avg acc (val): 0.0000\n",
      "----------\n",
      "\n",
      "Epoch 1/25\n",
      "----------\n",
      "Training batch 0/15.5\n",
      "Validation batch 0/5\n",
      "Epoch 1 result: \n",
      "Avg loss (train): 0.3493\n",
      "Avg acc (train): 0.0000\n",
      "Avg loss (val): 0.3176\n",
      "Avg acc (val): 0.0000\n",
      "----------\n",
      "\n",
      "Epoch 2/25\n",
      "----------\n",
      "Training batch 0/15.5\n",
      "Validation batch 0/5\n",
      "Epoch 2 result: \n",
      "Avg loss (train): 0.2960\n",
      "Avg acc (train): 0.0000\n",
      "Avg loss (val): 0.3514\n",
      "Avg acc (val): 0.0000\n",
      "----------\n",
      "\n",
      "Epoch 3/25\n",
      "----------\n",
      "Training batch 0/15.5\n",
      "Validation batch 0/5\n",
      "Epoch 3 result: \n",
      "Avg loss (train): 0.3152\n",
      "Avg acc (train): 0.0000\n",
      "Avg loss (val): 0.3257\n",
      "Avg acc (val): 0.0000\n",
      "----------\n",
      "\n",
      "Epoch 4/25\n",
      "----------\n",
      "Training batch 0/15.5\n",
      "Validation batch 0/5\n",
      "Epoch 4 result: \n",
      "Avg loss (train): 0.2771\n",
      "Avg acc (train): 0.0000\n",
      "Avg loss (val): 0.5266\n",
      "Avg acc (val): 0.0000\n",
      "----------\n",
      "\n",
      "Epoch 5/25\n",
      "----------\n",
      "Training batch 0/15.5\n",
      "Validation batch 0/5\n",
      "Epoch 5 result: \n",
      "Avg loss (train): 0.3484\n",
      "Avg acc (train): 0.0000\n",
      "Avg loss (val): 0.3325\n",
      "Avg acc (val): 0.0000\n",
      "----------\n",
      "\n",
      "Epoch 6/25\n",
      "----------\n",
      "Training batch 0/15.5\n",
      "Validation batch 0/5\n",
      "Epoch 6 result: \n",
      "Avg loss (train): 0.3210\n",
      "Avg acc (train): 0.0000\n",
      "Avg loss (val): 0.3500\n",
      "Avg acc (val): 0.0000\n",
      "----------\n",
      "\n",
      "Epoch 7/25\n",
      "----------\n",
      "Training batch 0/15.5\n",
      "Validation batch 0/5\n",
      "Epoch 7 result: \n",
      "Avg loss (train): 0.2684\n",
      "Avg acc (train): 0.0000\n",
      "Avg loss (val): 0.3036\n",
      "Avg acc (val): 0.0000\n",
      "----------\n",
      "\n",
      "Epoch 8/25\n",
      "----------\n",
      "Training batch 0/15.5\n",
      "Validation batch 0/5\n",
      "Epoch 8 result: \n",
      "Avg loss (train): 0.2808\n",
      "Avg acc (train): 0.0000\n",
      "Avg loss (val): 0.3169\n",
      "Avg acc (val): 0.0000\n",
      "----------\n",
      "\n",
      "Epoch 9/25\n",
      "----------\n",
      "Training batch 0/15.5\n",
      "Validation batch 0/5\n",
      "Epoch 9 result: \n",
      "Avg loss (train): 0.3152\n",
      "Avg acc (train): 0.0000\n",
      "Avg loss (val): 0.3081\n",
      "Avg acc (val): 0.0000\n",
      "----------\n",
      "\n",
      "Epoch 10/25\n",
      "----------\n",
      "Training batch 0/15.5\n",
      "Validation batch 0/5\n",
      "Epoch 10 result: \n",
      "Avg loss (train): 0.2899\n",
      "Avg acc (train): 0.0000\n",
      "Avg loss (val): 0.3210\n",
      "Avg acc (val): 0.0000\n",
      "----------\n",
      "\n",
      "Epoch 11/25\n",
      "----------\n",
      "Training batch 0/15.5\n",
      "Validation batch 0/5\n",
      "Epoch 11 result: \n",
      "Avg loss (train): 0.3155\n",
      "Avg acc (train): 0.0000\n",
      "Avg loss (val): 0.3059\n",
      "Avg acc (val): 0.0000\n",
      "----------\n",
      "\n",
      "Epoch 12/25\n",
      "----------\n",
      "Training batch 0/15.5\n",
      "Validation batch 0/5\n",
      "Epoch 12 result: \n",
      "Avg loss (train): 0.3089\n",
      "Avg acc (train): 0.0000\n",
      "Avg loss (val): 0.3059\n",
      "Avg acc (val): 0.0000\n",
      "----------\n",
      "\n",
      "Epoch 13/25\n",
      "----------\n",
      "Training batch 0/15.5\n",
      "Validation batch 0/5\n",
      "Epoch 13 result: \n",
      "Avg loss (train): 0.3122\n",
      "Avg acc (train): 0.0000\n",
      "Avg loss (val): 0.3086\n",
      "Avg acc (val): 0.0000\n",
      "----------\n",
      "\n",
      "Epoch 14/25\n",
      "----------\n",
      "Training batch 0/15.5\n",
      "Validation batch 0/5\n",
      "Epoch 14 result: \n",
      "Avg loss (train): 0.2887\n",
      "Avg acc (train): 0.0000\n",
      "Avg loss (val): 0.3088\n",
      "Avg acc (val): 0.0000\n",
      "----------\n",
      "\n",
      "Epoch 15/25\n",
      "----------\n",
      "Training batch 0/15.5\n",
      "Validation batch 0/5\n",
      "Epoch 15 result: \n",
      "Avg loss (train): 0.3037\n",
      "Avg acc (train): 0.0000\n",
      "Avg loss (val): 0.3044\n",
      "Avg acc (val): 0.0000\n",
      "----------\n",
      "\n",
      "Epoch 16/25\n",
      "----------\n",
      "Training batch 0/15.5\n",
      "Validation batch 0/5\n",
      "Epoch 16 result: \n",
      "Avg loss (train): 0.2973\n",
      "Avg acc (train): 0.0000\n",
      "Avg loss (val): 0.3114\n",
      "Avg acc (val): 0.0000\n",
      "----------\n",
      "\n",
      "Epoch 17/25\n",
      "----------\n",
      "Training batch 0/15.5\n",
      "Validation batch 0/5\n",
      "Epoch 17 result: \n",
      "Avg loss (train): 0.2933\n",
      "Avg acc (train): 0.0000\n",
      "Avg loss (val): 0.3132\n",
      "Avg acc (val): 0.0000\n",
      "----------\n",
      "\n",
      "Epoch 18/25\n",
      "----------\n",
      "Training batch 0/15.5\n",
      "Validation batch 0/5\n",
      "Epoch 18 result: \n",
      "Avg loss (train): 0.2988\n",
      "Avg acc (train): 0.0000\n",
      "Avg loss (val): 0.3167\n",
      "Avg acc (val): 0.0000\n",
      "----------\n",
      "\n",
      "Epoch 19/25\n",
      "----------\n",
      "Training batch 0/15.5\n",
      "Validation batch 0/5\n",
      "Epoch 19 result: \n",
      "Avg loss (train): 0.2885\n",
      "Avg acc (train): 0.0000\n",
      "Avg loss (val): 0.3061\n",
      "Avg acc (val): 0.0000\n",
      "----------\n",
      "\n",
      "Epoch 20/25\n",
      "----------\n",
      "Training batch 0/15.5\n",
      "Validation batch 0/5\n",
      "Epoch 20 result: \n",
      "Avg loss (train): 0.3014\n",
      "Avg acc (train): 0.0000\n",
      "Avg loss (val): 0.3080\n",
      "Avg acc (val): 0.0000\n",
      "----------\n",
      "\n",
      "Epoch 21/25\n",
      "----------\n",
      "Training batch 0/15.5\n",
      "Validation batch 0/5\n",
      "Epoch 21 result: \n",
      "Avg loss (train): 0.3052\n",
      "Avg acc (train): 0.0000\n",
      "Avg loss (val): 0.3041\n",
      "Avg acc (val): 0.0000\n",
      "----------\n",
      "\n",
      "Epoch 22/25\n",
      "----------\n",
      "Training batch 0/15.5\n",
      "Validation batch 0/5\n",
      "Epoch 22 result: \n",
      "Avg loss (train): 0.2895\n",
      "Avg acc (train): 0.0000\n",
      "Avg loss (val): 0.3120\n",
      "Avg acc (val): 0.0000\n",
      "----------\n",
      "\n",
      "Epoch 23/25\n",
      "----------\n",
      "Training batch 0/15.5\n",
      "Validation batch 0/5\n",
      "Epoch 23 result: \n",
      "Avg loss (train): 0.2972\n",
      "Avg acc (train): 0.0000\n",
      "Avg loss (val): 0.3112\n",
      "Avg acc (val): 0.0000\n",
      "----------\n",
      "\n",
      "Epoch 24/25\n",
      "----------\n",
      "Training batch 0/15.5\n",
      "Validation batch 0/5\n",
      "Epoch 24 result: \n",
      "Avg loss (train): 0.2934\n",
      "Avg acc (train): 0.0000\n",
      "Avg loss (val): 0.3249\n",
      "Avg acc (val): 0.0000\n",
      "----------\n",
      "\n",
      "\n",
      "Training completed in 4m 43s\n",
      "Best acc: 0.0000\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gpu_dtype = torch.cuda.FloatTensor\n",
    "\n",
    "def get_vgg19(num_classes):\n",
    "    net = models.vgg19()\n",
    "    net.classifier = nn.Sequential(\n",
    "        nn.Linear(512 * 7 * 7, 4096),\n",
    "        nn.ReLU(True),\n",
    "        nn.Dropout(),\n",
    "        nn.Linear(4096, 4096),\n",
    "        nn.ReLU(True),\n",
    "        nn.Dropout(),\n",
    "        nn.Linear(4096, num_classes),\n",
    "    )\n",
    "    return net.type(gpu_dtype)\n",
    "\n",
    "vgg19 = get_vgg19(6)\n",
    "vgg19.cuda()\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(vgg19.parameters(), lr=0.001, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "#train(vgg19, loss_fn, optimizer, num_epochs=1)\n",
    "vgg16 = train_model(vgg19, loss_fn, optimizer, exp_lr_scheduler, num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model\n",
      "----------\n",
      "Test batch 0/5\n",
      "Evaluation completed in 0m 1s\n",
      "Avg loss (test): 0.3587\n",
      "Avg acc (test): 0.0000\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "eval_model(vgg19, loss_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
